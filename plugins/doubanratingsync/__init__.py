from datetime import datetime, timedelta
import sqlite3
import json
import re
import pytz
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from app.schemas.types import EventType, NotificationType
from app.core.event import eventmanager, Event
from pathlib import Path
from app.core.config import settings
from app.plugins import _PluginBase
from typing import Any, List, Dict, Tuple, Optional, Set
from app.log import logger
import time
from urllib.parse import unquote, quote
import requests
from bs4 import BeautifulSoup
from http.cookies import SimpleCookie
from app.helper.cookiecloud import CookieCloudHelper


class DoubanHelper:
    """è±†ç“£å·¥å…·ç±»ï¼ˆä¼˜åŒ–ï¼šæ”¯æŒIDæŸ¥è¯¢+æ™ºèƒ½é‡è¯•ï¼‰"""
    def __init__(self, user_cookie: str = None):
        self.cookies: dict = {}
        # åˆå§‹åŒ–Cookie
        if not user_cookie:
            self.cookiecloud = CookieCloudHelper()
            cookie_dict, msg = self.cookiecloud.download()
            if cookie_dict is None:
                logger.error(f"è·å–CookieCloudæ•°æ®å¤±è´¥ï¼š{msg}")
            else:
                self.cookies = cookie_dict.get("douban.com", {})
        else:
            try:
                self.cookies = {k: v.value for k, v in SimpleCookie(user_cookie).items()}
            except Exception as e:
                logger.error(f"è§£æç”¨æˆ·ä¼ å…¥Cookieå¤±è´¥ï¼š{e}")
                self.cookies = {}

        # åˆå§‹åŒ–è¯·æ±‚å¤´
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'
        self.headers = {
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, sdch',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4',
            'Connection': 'keep-alive',
            'DNT': '1'
        }

        # æ—¥å¿—è¾“å‡ºCookieçŠ¶æ€
        if not self.cookies:
            logger.warning("è±†ç“£Cookieä¸ºç©ºï¼Œå¯èƒ½å¯¼è‡´æŸ¥è¯¢å¤±è´¥æˆ–é¢‘ç‡é™åˆ¶ï¼Œå»ºè®®é…ç½®Cookie")

    def get_rating_by_id(self, imdb_id: str = None, tmdb_id: str = None) -> Optional[str]:
        """é€šè¿‡IMDb/TMDB IDæŸ¥è¯¢è±†ç“£è¯„åˆ†ï¼ˆä¼˜å…ˆIDåŒ¹é…ï¼Œæ›´ç²¾å‡†ï¼‰"""
        if not imdb_id and not tmdb_id:
            return None

        # æ„é€ æœç´¢URLï¼ˆä¼˜å…ˆIMDb IDï¼‰
        if imdb_id:
            url = f"https://www.douban.com/search?cat=1002&q={quote(imdb_id)}"
            logger.debug(f"é€šè¿‡IMDb IDæŸ¥è¯¢è±†ç“£ï¼š{imdb_id} â†’ URLï¼š{url}")
        else:
            url = f"https://www.douban.com/search?cat=1002&q=tmdb{tmdb_id}"
            logger.debug(f"é€šè¿‡TMDB IDæŸ¥è¯¢è±†ç“£ï¼š{tmdb_id} â†’ URLï¼š{url}")

        try:
            response = requests.get(
                url,
                headers=self.headers,
                cookies=self.cookies,
                timeout=15,
                allow_redirects=True
            )
            response.raise_for_status()

            # çŠ¶æ€ç å¤„ç†
            if response.status_code == 429:
                logger.warning(f"è±†ç“£æ¥å£é™æµï¼ˆIDæŸ¥è¯¢ï¼‰ï¼š{imdb_id or tmdb_id}")
                return None
            elif response.status_code != 200:
                logger.error(f"è±†ç“£IDæŸ¥è¯¢çŠ¶æ€ç å¼‚å¸¸ï¼š{response.status_code}ï¼ˆIDï¼š{imdb_id or tmdb_id}ï¼‰")
                return None

            # è§£æè¯„åˆ†
            soup = BeautifulSoup(response.text, 'html.parser')
            rating_span = soup.find("span", class_="rating_nums")
            if rating_span:
                score = rating_span.get_text(strip=True)
                return score if score != "0" else None
            else:
                logger.debug(f"IDæŸ¥è¯¢æœªæ‰¾åˆ°è¯„åˆ†ï¼š{imdb_id or tmdb_id}")
                return None

        except requests.exceptions.RequestException as e:
            logger.error(f"IDæŸ¥è¯¢è±†ç“£å¤±è´¥ï¼ˆ{imdb_id or tmdb_id}ï¼‰ï¼š{str(e)[:50]}")
            return None

    def get_rating_by_title(self, title: str, year: str = None) -> Optional[str]:
        """é€šè¿‡æ ‡é¢˜+å¹´ä»½æŸ¥è¯¢è±†ç“£è¯„åˆ†ï¼ˆå…¼å®¹æ— IDåœºæ™¯ï¼‰"""
        # æ ‡é¢˜+å¹´ä»½ç»„åˆæœç´¢ï¼Œæå‡å‡†ç¡®ç‡
        search_keyword = f"{title} {year}" if year else title
        encoded_title = quote(search_keyword, safe='')
        url = f"https://www.douban.com/search?cat=1002&q={encoded_title}"
        logger.debug(f"é€šè¿‡æ ‡é¢˜æŸ¥è¯¢è±†ç“£ï¼š{search_keyword} â†’ URLï¼š{url}")

        try:
            response = requests.get(
                url,
                headers=self.headers,
                cookies=self.cookies,
                timeout=15,
                allow_redirects=True
            )
            response.raise_for_status()

            if response.status_code == 429:
                logger.warning(f"è±†ç“£æ¥å£é™æµï¼ˆæ ‡é¢˜æŸ¥è¯¢ï¼‰ï¼š{search_keyword}")
                return None
            elif response.status_code != 200:
                logger.error(f"è±†ç“£æ ‡é¢˜æŸ¥è¯¢çŠ¶æ€ç å¼‚å¸¸ï¼š{response.status_code}ï¼ˆå…³é”®è¯ï¼š{search_keyword}ï¼‰")
                return None

            # è§£ææœç´¢ç»“æœï¼ˆä¼˜å…ˆåŒ¹é…å¹´ä»½ï¼‰
            soup = BeautifulSoup(response.text, 'html.parser')
            title_divs = soup.find_all("div", class_="title")
            subject_items = []

            for div in title_divs:
                a_tag = div.find("a")
                if not a_tag:
                    continue

                item = {}
                item["title"] = a_tag.get_text(strip=True)
                # æå–å¹´ä»½
                cast_span = div.find(class_="subject-cast")
                item["year"] = re.search(r'(\d{4})$', cast_span.get_text(strip=True)).group(1) if cast_span else ""
                # æå–è¯„åˆ†
                rating_span = div.find(class_="rating_nums")
                item["score"] = rating_span.get_text(strip=True) if rating_span else None
                subject_items.append(item)

            # ä¼˜å…ˆåŒ¹é…å¹´ä»½
            if year:
                for item in subject_items:
                    if item["year"] == year and item["score"] and item["score"] != "0":
                        return item["score"]
            # æ— å¹´ä»½åŒ¹é…åˆ™è¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆè¯„åˆ†
            for item in subject_items:
                if item["score"] and item["score"] != "0":
                    return item["score"]

            logger.debug(f"æ ‡é¢˜æŸ¥è¯¢æœªæ‰¾åˆ°æœ‰æ•ˆè¯„åˆ†ï¼š{search_keyword}")
            return None

        except requests.exceptions.RequestException as e:
            logger.error(f"æ ‡é¢˜æŸ¥è¯¢è±†ç“£å¤±è´¥ï¼ˆ{search_keyword}ï¼‰ï¼š{str(e)[:50]}")
            return None

    def get_rating(self, meta_info: dict) -> Optional[str]:
        """ç»Ÿä¸€è¯„åˆ†æŸ¥è¯¢å…¥å£ï¼šå…ˆIDåæ ‡é¢˜ï¼Œæ™ºèƒ½åŒ¹é…"""
        # ä»meta_infoæå–IMDb/TMDB IDå’Œå¹´ä»½
        imdb_id = meta_info.get("imdb_id")
        tmdb_id = meta_info.get("tmdb_id")
        year = str(meta_info.get("year", "")) if meta_info.get("year") else None
        title = meta_info.get("title", "")

        # 1. ä¼˜å…ˆé€šè¿‡IMDb IDæŸ¥è¯¢
        if imdb_id and imdb_id.startswith("tt"):
            score = self.get_rating_by_id(imdb_id=imdb_id)
            if score:
                return score
        # 2. å…¶æ¬¡é€šè¿‡TMDB IDæŸ¥è¯¢
        if tmdb_id:
            score = self.get_rating_by_id(tmdb_id=str(tmdb_id))
            if score:
                return score
        # 3. æœ€åé€šè¿‡æ ‡é¢˜+å¹´ä»½æŸ¥è¯¢
        score = self.get_rating_by_title(title=title, year=year)
        return score


class DoubanRatingSync(_PluginBase):
    # æ’ä»¶åŸºç¡€ä¿¡æ¯ï¼ˆå”¯ä¸€æ ‡è¯†ï¼Œé¿å…å†²çªï¼‰
    plugin_name = "è±†ç“£è¯„åˆ†ä¿®æ­£"
    plugin_desc = "åŒæ­¥è±†ç“£è¯„åˆ†è‡³æå½±è§†ï¼ˆä¼˜å…ˆIDåŒ¹é…ï¼Œæ— è¯„åˆ†ä¿ç•™åŸæ•°æ®ï¼Œæ”¯æŒè¿›åº¦è¿½è¸ªï¼‰"
    plugin_icon = "https://img9.doubanio.com/favicon.ico"
    plugin_version = "1.2"  # ä¼˜åŒ–ç‰ˆç‰ˆæœ¬å·
    plugin_author = "funcygo"
    author_url = "https://github.com/funcygo"
    plugin_config_prefix = "doubanratingsync"
    plugin_order = 10
    auth_level = 1

    # ç§æœ‰å±æ€§ï¼ˆæ–°å¢é…ç½®é¡¹ï¼‰
    _enabled = False
    _cron = "0 1 * * *"  # æ¯å¤©å‡Œæ™¨1ç‚¹æ‰§è¡Œ
    _notify = True
    _onlyonce = False
    _db_path = ""
    _cookie = ""
    _douban_score_update_days = 30  # é»˜è®¤30å¤©æ›´æ–°
    _max_sync_count = 100  # å•æ¬¡åŒæ­¥æœ€å¤§å½±ç‰‡æ•°ï¼ˆæ–°å¢ï¼‰
    _sync_types: Set[str] = {"movie", "tv"}  # åŒæ­¥ç±»å‹ï¼šç”µå½±/ç”µè§†å‰§ï¼ˆæ–°å¢ï¼‰
    _cached_data: dict = {}  # ç¼“å­˜ï¼šæ ‡é¢˜â†’{score, time}
    _scheduler: Optional[BackgroundScheduler] = None
    _should_stop = False
    _douban_helper: Optional[DoubanHelper] = None

    def init_plugin(self, config: dict = None):
        self._should_stop = False
        self.stop_service()  # åœæ­¢ç°æœ‰ä»»åŠ¡ï¼Œé¿å…é‡å¤

        # åŠ è½½é…ç½®
        if config:
            self._enabled = config.get("enabled", False)
            self._cron = config.get("cron", self._cron)
            self._notify = config.get("notify", True)
            self._onlyonce = config.get("onlyonce", False)
            self._db_path = config.get("db_path", "")
            self._cookie = config.get("cookie", "")
            self._douban_score_update_days = int(config.get("douban_score_update_days", 30))
            self._max_sync_count = int(config.get("max_sync_count", 100))
            self._sync_types = set(config.get("sync_types", ["movie", "tv"]))  # è½¬ä¸ºé›†åˆ
            # åˆå§‹åŒ–è±†ç“£å·¥å…·ç±»
            self._douban_helper = DoubanHelper(user_cookie=self._cookie)

        # åŠ è½½å¹¶æ¸…ç†ç¼“å­˜ï¼ˆæ¸…ç†è¶…è¿‡365å¤©çš„è®°å½•ï¼‰
        self._cached_data = self.get_data("doubanratingsync") or {}
        self._clean_expired_cache()
        logger.info(f"ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œæœ‰æ•ˆç¼“å­˜æ•°ï¼š{len(self._cached_data)}")

        # æ ¡éªŒæ•°æ®åº“è·¯å¾„
        if self._onlyonce or (self._enabled and self._cron):
            path = Path(self._db_path)
            if not path.exists():
                logger.error(f"æå½±è§†æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨ï¼š{self._db_path}")
                if self._notify:
                    self.post_message(
                        mtype=NotificationType.SiteMessage,
                        title="ã€è±†ç“£è¯„åˆ†ä¿®æ­£ã€‘åˆå§‹åŒ–å¤±è´¥",
                        text=f"æå½±è§†æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨ï¼š{self._db_path}\nè¯·æ£€æŸ¥è·¯å¾„é…ç½®å¹¶æ˜ å°„æ•°æ®åº“æ–‡ä»¶"
                    )
                return

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        if self._onlyonce:
            logger.info("è±†ç“£è¯„åˆ†ä¿®æ­£ï¼šç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥ä»»åŠ¡")
            self._scheduler = BackgroundScheduler(timezone=settings.TZ)
            self._scheduler.add_job(
                func=self.sync_douban_rating,
                trigger="date",
                run_date=datetime.now(tz=pytz.timezone(settings.TZ)) + timedelta(seconds=3),
                name="è±†ç“£è¯„åˆ†ä¿®æ­£-ç«‹å³æ‰§è¡Œ"
            )
            # å…³é—­ç«‹å³æ‰§è¡Œå¼€å…³
            self._onlyonce = False
            self._update_config()
            # å¯åŠ¨ä»»åŠ¡
            if self._scheduler.get_jobs():
                self._scheduler.start()

    def _clean_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆè¶…è¿‡365å¤©ï¼‰"""
        current_time = datetime.now()
        expired_keys = []
        for title, cache_info in self._cached_data.items():
            try:
                cache_time = datetime.strptime(cache_info.get("time", ""), "%Y-%m-%d %H:%M:%S")
                if (current_time - cache_time).days > 365:
                    expired_keys.append(title)
            except:
                expired_keys.append(title)  # æ ¼å¼å¼‚å¸¸çš„ç¼“å­˜ä¹Ÿæ¸…ç†

        if expired_keys:
            for key in expired_keys:
                del self._cached_data[key]
            self.save_data("doubanratingsync", self._cached_data)
            logger.info(f"æ¸…ç†è¿‡æœŸç¼“å­˜ï¼š{len(expired_keys)} æ¡")

    def get_state(self) -> bool:
        return self._enabled

    def _update_config(self):
        """æ›´æ–°é…ç½®åˆ°æ•°æ®åº“"""
        self.update_config({
            "enabled": self._enabled,
            "cron": self._cron,
            "notify": self._notify,
            "onlyonce": self._onlyonce,
            "db_path": self._db_path,
            "cookie": self._cookie,
            "douban_score_update_days": self._douban_score_update_days,
            "max_sync_count": self._max_sync_count,
            "sync_types": list(self._sync_types)  # é›†åˆè½¬åˆ—è¡¨å­˜å‚¨
        })

    def get_command(self) -> List[Dict[str, Any]]:
        """æ³¨å†Œæ‰‹åŠ¨å‘½ä»¤"""
        return [
            {
                "cmd": "/sync_douban_rating",
                "event": EventType.PluginAction,
                "desc": "æ‰‹åŠ¨åŒæ­¥è±†ç“£è¯„åˆ†è‡³æå½±è§†",
                "category": "å·¥å…·",
                "data": {"action": "sync_douban_rating"}
            }
        ]

    @eventmanager.register(EventType.PluginAction)
    def handle_command(self, event: Event):
        """å¤„ç†æ‰‹åŠ¨å‘½ä»¤"""
        event_data = event.event_data or {}
        if event_data.get("action") == "sync_douban_rating":
            logger.info("æ”¶åˆ°æ‰‹åŠ¨å‘½ä»¤ï¼šåŒæ­¥è±†ç“£è¯„åˆ†")
            self.post_message(
                channel=event.event_data.get("channel"),
                title="ã€è±†ç“£è¯„åˆ†ä¿®æ­£ã€‘",
                text="å¼€å§‹åŒæ­¥è±†ç“£è¯„åˆ†è‡³æå½±è§†...",
                userid=event.event_data.get("user")
            )
            # æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
            self.sync_douban_rating()
            self.post_message(
                channel=event.event_data.get("channel"),
                title="ã€è±†ç“£è¯„åˆ†ä¿®æ­£ã€‘",
                text="è±†ç“£è¯„åˆ†åŒæ­¥ä»»åŠ¡å·²å®Œæˆï¼å¯æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…",
                userid=event.event_data.get("user")
            )

    def get_api(self) -> List[Dict[str, Any]]:
        pass

    def get_service(self) -> List[Dict[str, Any]]:
        """æ³¨å†Œå…¬å…±å®šæ—¶æœåŠ¡"""
        if self._enabled and self._cron:
            return [
                {
                    "id": "DoubanRatingSync",
                    "name": "è±†ç“£è¯„åˆ†ä¿®æ­£",
                    "trigger": CronTrigger.from_crontab(self._cron),
                    "func": self.sync_douban_rating,
                    "kwargs": {}
                }
            ]
        return []

    def _get_media_type(self, meta_info: dict) -> Optional[str]:
        """è¯†åˆ«å½±ç‰‡ç±»å‹ï¼ˆmovie/tv/otherï¼‰"""
        media_type = meta_info.get("type")
        if media_type == "ç”µå½±":
            return "movie"
        elif media_type in ["ç”µè§†å‰§", "ç»¼è‰º", "åŠ¨æ¼«"]:  # ç»¼è‰º/åŠ¨æ¼«å½’ä¸ºtvç±»
            return "tv"
        else:
            return "other"

    def sync_douban_rating(self):
        """æ ¸å¿ƒåŒæ­¥é€»è¾‘ï¼ˆå…¨ä¼˜åŒ–ç‰ˆï¼‰"""
        self._should_stop = False
        logger.info("="*60 + " è±†ç“£è¯„åˆ†ä¿®æ­£åŒæ­¥å¼€å§‹ " + "="*60)
        message = ""

        # è¿›åº¦ç»Ÿè®¡å˜é‡
        total_query = 0  # æŸ¥è¯¢åˆ°çš„æœ‰æ•ˆå½±ç‰‡æ€»æ•°
        total_filtered = 0  # ç­›é€‰åç¬¦åˆæ¡ä»¶ï¼ˆç±»å‹+éœ€æ›´æ–°ï¼‰çš„æ€»æ•°
        updated_count = 0  # æˆåŠŸæ›´æ–°æ•°é‡
        skipped_count = 0  # è·³è¿‡æ•°é‡
        skipped_logs = []  # è·³è¿‡æ˜ç»†
        failed_count = 0  # å¤±è´¥æ•°é‡
        failed_logs = []  # å¤±è´¥æ˜ç»†

        try:
            # 1. æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨å…³é—­ï¼‰
            with sqlite3.connect(self._db_path) as conn:
                conn.text_factory = str
                cursor = conn.cursor()

                # 2. æŸ¥è¯¢å¾…å¤„ç†å½±ç‰‡ï¼ˆSQLä¼˜åŒ–ï¼šä»…æŸ¥è¯¢å¿…è¦å­—æ®µï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
                cursor.execute("""
                SELECT DISTINCT zc.rowid, zc.meta_info, zc.updated_at
                FROM zvideo_collection zc 
                LEFT JOIN zvideo_list zl ON zc.collection_id = zl.collection_id 
                WHERE zc.extend_type != 7  -- æ’é™¤åˆé›†
                  AND JSON_EXTRACT(zc.meta_info, '$.douban_score') IS NOT NULL  -- å«douban_scoreå­—æ®µ
                  AND (zl.path NOT LIKE '/tmp/zfsv3/sata11/13107640652/data/RR%' OR zl.path IS NULL)  -- è¿‡æ»¤æ— æ•ˆè·¯å¾„
                LIMIT ?  -- é™åˆ¶å•æ¬¡æŸ¥è¯¢æ•°é‡
                """, (self._max_sync_count,))
                rows = cursor.fetchall()
                total_query = len(rows)
                logger.info(f"ã€æŸ¥è¯¢ç»“æœã€‘å…±æŸ¥è¯¢åˆ° {total_query} éƒ¨æœ‰æ•ˆå½±ç‰‡ï¼ˆå·²å»é‡ï¼Œå•æ¬¡ä¸Šé™ï¼š{self._max_sync_count}ï¼‰")

                if not rows:
                    logger.info("ã€æ— ä»»åŠ¡ã€‘æ— éœ€è¦å¤„ç†çš„å½±ç‰‡ï¼Œä»»åŠ¡ç»“æŸ")
                    return

                # 3. ç­›é€‰ç¬¦åˆæ¡ä»¶çš„å½±ç‰‡ï¼ˆç±»å‹+éœ€æ›´æ–°ï¼‰
                current_time = datetime.now()
                batch_update = []
                for row in rows:
                    if self._should_stop:
                        logger.info("ã€ä»»åŠ¡ä¸­æ–­ã€‘æ£€æµ‹åˆ°ä¸­æ–­è¯·æ±‚ï¼Œåœæ­¢åŒæ­¥")
                        return

                    rowid, meta_info_json, updated_at = row
                    try:
                        # è§£æmeta_infoï¼ˆå®¹é”™ï¼šå­—æ®µç¼ºå¤±å¤„ç†ï¼‰
                        meta_info = json.loads(meta_info_json)
                        title = meta_info.get("title", f"æœªçŸ¥å½±ç‰‡ï¼ˆrowidï¼š{rowid}ï¼‰")
                        old_score = float(meta_info.get("douban_score", 0))
                        media_type = self._get_media_type(meta_info)

                        # ç±»å‹ç­›é€‰ï¼šè·³è¿‡æœªå‹¾é€‰çš„ç±»å‹
                        if media_type not in self._sync_types:
                            skipped_count += 1
                            skip_reason = f"ã€è·³è¿‡-ç±»å‹ä¸åŒ¹é…ã€‘{title}ï¼šç±»å‹{media_type}æœªå‹¾é€‰åŒæ­¥"
                            logger.info(skip_reason)
                            skipped_logs.append(skip_reason)
                            continue

                        # ç¼“å­˜æ£€æŸ¥ï¼šæ˜¯å¦éœ€è¦æ›´æ–°
                        cache_info = self._cached_data.get(title, {})
                        need_update = False
                        if not cache_info:
                            need_update = True
                            logger.debug(f"ã€ç¼“å­˜æœªå‘½ä¸­ã€‘{title}ï¼šé¦–æ¬¡å¤„ç†ï¼Œéœ€æ›´æ–°")
                        else:
                            cache_score = cache_info.get("score")
                            try:
                                cache_time = datetime.strptime(cache_info.get("time", ""), "%Y-%m-%d %H:%M:%S")
                                # ç¼“å­˜è¿‡æœŸæˆ–è¯„åˆ†å˜åŒ–éœ€æ›´æ–°
                                if (current_time - cache_time).days >= self._douban_score_update_days or str(cache_score) != str(old_score):
                                    need_update = True
                                    logger.debug(f"ã€ç¼“å­˜è¿‡æœŸ/è¯„åˆ†å˜åŒ–ã€‘{title}ï¼šç¼“å­˜æ—¶é—´{cache_time.strftime('%Y-%m-%d')}ï¼Œéœ€æ›´æ–°")
                                else:
                                    skipped_count += 1
                                    skip_reason = f"ã€è·³è¿‡-ç¼“å­˜æœªè¿‡æœŸã€‘{title}ï¼šç¼“å­˜æ›´æ–°äº{cache_time.strftime('%Y-%m-%d')}ï¼Œè¯„åˆ†{old_score}æœªå˜åŒ–ï¼ˆæœ‰æ•ˆæœŸè‡³{cache_time.strftime('%Y-%m-%d')}ï¼‰"
                                    logger.info(skip_reason)
                                    skipped_logs.append(skip_reason)
                            except:
                                # ç¼“å­˜æ ¼å¼å¼‚å¸¸ï¼Œè§†ä¸ºéœ€è¦æ›´æ–°
                                need_update = True
                                logger.debug(f"ã€ç¼“å­˜æ ¼å¼å¼‚å¸¸ã€‘{title}ï¼šé‡æ–°è·å–è¯„åˆ†")

                        if need_update:
                            batch_update.append((rowid, title, old_score, meta_info))
                    except json.JSONDecodeError as e:
                        # JSONè§£æå¤±è´¥ï¼šè·³è¿‡å¹¶è®°å½•
                        skipped_count += 1
                        skip_reason = f"ã€è·³è¿‡-è§£æå¤±è´¥ã€‘rowidï¼š{rowid}ï¼ŒJSONè§£æé”™è¯¯ï¼š{str(e)[:30]}"
                        logger.error(skip_reason)
                        skipped_logs.append(skip_reason)
                    except Exception as e:
                        # å…¶ä»–å¼‚å¸¸ï¼šè·³è¿‡å¹¶è®°å½•
                        skipped_count += 1
                        skip_reason = f"ã€è·³è¿‡-æœªçŸ¥é”™è¯¯ã€‘{title}ï¼ˆrowidï¼š{rowid}ï¼‰ï¼š{str(e)[:30]}"
                        logger.error(skip_reason)
                        skipped_logs.append(skip_reason)

                total_filtered = len(batch_update)
                logger.info(f"\nã€ç­›é€‰ç»“æœã€‘ç¬¦åˆæ¡ä»¶éœ€æ›´æ–°å½±ç‰‡ï¼š{total_filtered} éƒ¨ï¼Œå·²è·³è¿‡ï¼š{skipped_count} éƒ¨")
                if not batch_update:
                    logger.info("ã€æ— æ›´æ–°ä»»åŠ¡ã€‘æ‰€æœ‰å½±ç‰‡å‡æ— éœ€æ›´æ–°ï¼Œä»»åŠ¡ç»“æŸ")
                    return

                # 4. æ‰¹é‡è·å–è±†ç“£è¯„åˆ†ï¼ˆæ™ºèƒ½é‡è¯•+é™æµï¼‰
                batch_size = 10  # æ¯æ‰¹10éƒ¨
                title_score_map = {}
                tz = pytz.timezone(settings.TZ)
                current_time_str = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S.%f") + \
                                   datetime.now(tz).strftime("%z")[:3] + ":" + \
                                   datetime.now(tz).strftime("%z")[3:]

                for i in range(0, len(batch_update), batch_size):
                    if self._should_stop:
                        return

                    batch = batch_update[i:i+batch_size]
                    batch_idx = i//batch_size + 1
                    processed_in_batch = 0  # æœ¬æ‰¹æ›´æ–°æ•°
                    logger.info(f"\nã€æ‰¹å¤„ç†ã€‘å¼€å§‹ç¬¬ {batch_idx} æ‰¹ï¼Œæœ¬æ‰¹ {len(batch)} éƒ¨å½±ç‰‡ï¼ˆæ€»è¿›åº¦ï¼š{updated_count}/{total_filtered}ï¼‰")

                    for rowid, title, old_score, meta_info in batch:
                        remaining = total_filtered - updated_count
                        logger.info(f"ã€è·å–è¯„åˆ†ã€‘[{updated_count+1}/{total_filtered}] æ­£åœ¨æŸ¥è¯¢ï¼š{title}ï¼ˆåŸè¯„åˆ†ï¼š{old_score}ï¼Œç±»å‹ï¼š{self._get_media_type(meta_info)}ï¼‰")

                        # ä¼˜å…ˆä»ç¼“å­˜è·å–ï¼ˆæ‰¹å†…å»é‡ï¼‰
                        if title in title_score_map:
                            score = title_score_map[title]
                        else:
                            # æ™ºèƒ½é‡è¯•ï¼šæ ¹æ®å“åº”è°ƒæ•´ç­–ç•¥
                            retry_count = 3
                            score = None
                            retry_delay = 2  # åˆå§‹é‡è¯•é—´éš”2ç§’
                            while retry_count > 0:
                                try:
                                    score = self._douban_helper.get_rating(meta_info)
                                    if score:
                                        break
                                    # æœªè·å–åˆ°è¯„åˆ†ï¼Œç›´æ¥é€€å‡ºé‡è¯•
                                    if not score:
                                        break
                                except Exception as e:
                                    logger.error(f"ã€è·å–å¤±è´¥ã€‘{title}ï¼ˆå‰©ä½™é‡è¯•ï¼š{retry_count-1}ï¼‰ï¼š{str(e)[:50]}")
                                    time.sleep(retry_delay)
                                    # æ¯æ¬¡é‡è¯•é—´éš”ç¿»å€ï¼ˆ2â†’4â†’8ç§’ï¼‰
                                    retry_delay *= 2
                                retry_count -= 1
                            title_score_map[title] = score  # å³ä½¿ä¸ºNoneä¹Ÿå­˜å‚¨ï¼Œé¿å…é‡å¤è¯·æ±‚

                        # è®¡ç®—æœ‰æ•ˆæœŸ
                        expire_time = current_time + timedelta(days=self._douban_score_update_days)
                        expire_str = expire_time.strftime("%Y-%m-%d")

                        # å¤„ç†ç»“æœ
                        if score:
                            new_score = float(score)
                            if new_score != old_score:
                                # è¯„åˆ†æ›´æ–°
                                logger.info(f"ã€è¯„åˆ†æ›´æ–°ã€‘[{updated_count+1}/{total_filtered}] {title}ï¼šåŸè¯„åˆ† {old_score} â†’ è±†ç“£è¯„åˆ† {new_score}ï¼ˆæœ‰æ•ˆæœŸè‡³ {expire_str}ï¼‰")
                                # è®°å½•åˆ°æ›´æ–°åˆ—è¡¨
                                title_score_map[title] = new_score
                                processed_in_batch += 1
                            else:
                                # è¯„åˆ†ä¸€è‡´ï¼Œè·³è¿‡
                                skipped_count += 1
                                skip_reason = f"ã€è·³è¿‡-è¯„åˆ†ä¸€è‡´ã€‘{title}ï¼šè±†ç“£è¯„åˆ†{new_score}ä¸åŸè¯„åˆ†ä¸€è‡´ï¼ˆæœ‰æ•ˆæœŸè‡³ {expire_str}ï¼‰"
                                logger.info(skip_reason)
                                skipped_logs.append(skip_reason)
                                title_score_map[title] = old_score
                        else:
                            # æœªè·å–åˆ°æœ‰æ•ˆè¯„åˆ†ï¼Œè·³è¿‡
                            skipped_count += 1
                            skip_reason = f"ã€è·³è¿‡-æ— æœ‰æ•ˆè¯„åˆ†ã€‘{title}ï¼šæœªæŸ¥è¯¢åˆ°æœ‰æ•ˆè±†ç“£è¯„åˆ†ï¼Œä¿ç•™åŸè¯„åˆ†{old_score}ï¼ˆæœ‰æ•ˆæœŸè‡³ {expire_str}ï¼‰"
                            logger.info(skip_reason)
                            skipped_logs.append(skip_reason)
                            title_score_map[title] = old_score

                    # æ‰¹é—´é™æµï¼šé¿å…è§¦å‘è±†ç“£é¢‘ç‡é™åˆ¶
                    if i + batch_size < len(batch_update):
                        logger.info(f"ã€æ‰¹å¤„ç†å®Œæˆã€‘ç¬¬ {batch_idx} æ‰¹ç»“æŸï¼Œæœ¬æ‰¹æ›´æ–° {processed_in_batch} éƒ¨ï¼Œç´¯è®¡æ›´æ–° {updated_count} éƒ¨ï¼Œå‰©ä½™ {total_filtered - updated_count} éƒ¨")
                        logger.info("ã€é™æµå»¶è¿Ÿã€‘ç­‰å¾…10ç§’åç»§ç»­ä¸‹ä¸€æ‰¹...")
                        time.sleep(10)

                # 5. æ‰¹é‡æ›´æ–°æ•°æ®åº“ï¼ˆäº‹åŠ¡å®‰å…¨ï¼‰
                update_sql = """
                UPDATE zvideo_collection 
                SET meta_info = ?,  -- æ›´æ–°å«è±†ç“£è¯„åˆ†çš„meta_info
                    updated_at = ?,  -- æ›´æ–°æ—¶é—´
                    score = CAST(?) AS DECIMAL(3,1)  -- åŒæ­¥åˆ°scoreåˆ—ï¼ˆå‰ç«¯æ˜¾ç¤ºï¼‰
                WHERE rowid = ?
                """
                update_params = []

                for rowid, title, old_score, meta_info in batch_update:
                    new_score = title_score_map.get(title, old_score)
                    try:
                        new_score = float(new_score) if new_score else old_score
                    except:
                        new_score = old_score
                        failed_count += 1
                        failed_logs.append(f"ã€æ›´æ–°å¤±è´¥ã€‘{title}ï¼šè¯„åˆ†æ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•æ›´æ–°")
                        continue

                    # ä»…æ›´æ–°è¯„åˆ†æœ‰å˜åŒ–ä¸”æœ‰æ•ˆï¼ˆ>0ï¼‰çš„è®°å½•
                    if new_score != old_score and new_score > 0:
                        meta_info["douban_score"] = new_score
                        # è¡¥å……æ›´æ–°æ—¶é—´åˆ°meta_infoï¼ˆå¯é€‰ï¼‰
                        meta_info["douban_score_updated_at"] = current_time.strftime("%Y-%m-%d %H:%M:%S")
                        updated_meta = json.dumps(meta_info, ensure_ascii=False)
                        update_params.append((updated_meta, current_time_str, new_score, rowid))
                        updated_count += 1

                        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
                        expire_time = current_time + timedelta(days=self._douban_score_update_days)
                        expire_str = expire_time.strftime("%Y-%m-%d")
                        message += f"ğŸ”„ {title}ï¼šåŸè¯„åˆ† {old_score} â†’ è±†ç“£è¯„åˆ† {new_score}ï¼ˆæœ‰æ•ˆæœŸè‡³ {expire_str}ï¼‰\n"

                # æ‰§è¡Œæ‰¹é‡æ›´æ–°
                if update_params:
                    cursor.executemany(update_sql, update_params)
                    conn.commit()
                    logger.info(f"\nã€æ•°æ®åº“æ›´æ–°å®Œæˆã€‘å…±æ›´æ–° {len(update_params)} éƒ¨å½±ç‰‡è¯„åˆ†ï¼ˆç´¯è®¡æ›´æ–°ï¼š{updated_count} éƒ¨ï¼‰")

                    # æ›´æ–°ç¼“å­˜ï¼ˆåŒ…å«æœ¬æ¬¡æ‰€æœ‰å¤„ç†çš„å½±ç‰‡ï¼Œæ— è®ºæ˜¯å¦æ›´æ–°ï¼‰
                    for rowid, title, old_score, meta_info in batch_update:
                        final_score = title_score_map.get(title, old_score)
                        self._cached_data[title] = {
                            "score": final_score,
                            "time": current_time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    self.save_data("doubanratingsync", self._cached_data)
                else:
                    logger.info("ã€æ— æ›´æ–°ã€‘æ— éœ€è¦å†™å…¥æ•°æ®åº“çš„è¯„åˆ†å˜åŒ–")

        except sqlite3.Error as e:
            logger.error(f"\nã€æ•°æ®åº“é”™è¯¯ã€‘{str(e)}")
            failed_count += 1
            failed_logs.append(f"ã€æ•°æ®åº“é”™è¯¯ã€‘{str(e)}")
            if self._notify:
                self.post_message(
                    mtype=NotificationType.SiteMessage,
                    title="ã€è±†ç“£è¯„åˆ†ä¿®æ­£ã€‘åŒæ­¥å¤±è´¥",
                    text=f"æ•°æ®åº“æ“ä½œå¼‚å¸¸ï¼š{str(e)}"
                )
        except Exception as e:
            logger.error(f"\nã€å…¨å±€é”™è¯¯ã€‘åŒæ­¥ä»»åŠ¡å¼‚å¸¸ç»ˆæ­¢ï¼š{str(e)}")
            failed_count += 1
            failed_logs.append(f"ã€å…¨å±€é”™è¯¯ã€‘{str(e)}")
            if self._notify:
                self.post_message(
                    mtype=NotificationType.SiteMessage,
                    title="ã€è±†ç“£è¯„åˆ†ä¿®æ­£ã€‘åŒæ­¥å¤±è´¥",
                    text=f"ä»»åŠ¡å¼‚å¸¸ç»ˆæ­¢ï¼š{str(e)}"
                )

        # 6. ä»»åŠ¡æ±‡æ€»æ—¥å¿—ï¼ˆå®Œæ•´æ˜ç»†ï¼‰
        logger.info("\n" + "="*60 + " åŒæ­¥ä»»åŠ¡æ±‡æ€» " + "="*60)
        logger.info(f"æŸ¥è¯¢åˆ°æœ‰æ•ˆå½±ç‰‡ï¼š{total_query} éƒ¨")
        logger.info(f"ç¬¦åˆæ¡ä»¶éœ€æ›´æ–°å½±ç‰‡ï¼š{total_filtered} éƒ¨")
        logger.info(f"æˆåŠŸæ›´æ–°å½±ç‰‡ï¼š{updated_count} éƒ¨")
        logger.info(f"è·³è¿‡å½±ç‰‡ï¼š{skipped_count} éƒ¨")
        logger.info(f"å¤±è´¥å½±ç‰‡ï¼š{failed_count} éƒ¨")
        logger.info(f"æ›´æ–°ç‡ï¼š{updated_count/total_filtered*100:.1f}%" if total_filtered > 0 else "0.0%")

        # è·³è¿‡æ˜ç»†
        if skipped_logs:
            logger.info(f"\nã€è·³è¿‡å½±ç‰‡æ˜ç»†ã€‘ï¼ˆå…± {len(skipped_logs)} éƒ¨ï¼‰")
            for idx, log in enumerate(skipped_logs[:50], 1):  # æœ€å¤šæ˜¾ç¤º50æ¡ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                logger.info(f"{idx}. {log}")
            if len(skipped_logs) > 50:
                logger.info(f"... è¿˜æœ‰ {len(skipped_logs)-50} æ¡è·³è¿‡è®°å½•ï¼Œå¯æŸ¥çœ‹debugæ—¥å¿—")

        # å¤±è´¥æ˜ç»†
        if failed_logs:
            logger.info(f"\nã€å¤±è´¥å½±ç‰‡æ˜ç»†ã€‘ï¼ˆå…± {len(failed_logs)} éƒ¨ï¼‰")
            for idx, log in enumerate(failed_logs, 1):
                logger.info(f"{idx}. {log}")

        logger.info("="*60 + " åŒæ­¥ä»»åŠ¡ç»“æŸ " + "="*60)

        # å‘é€é€šçŸ¥ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        if self._notify and message:
            max_msg_len = 2000
            if len(message) > max_msg_len:
                message = message[:max_msg_len] + f"\n... è¿˜æœ‰ {len(message)-max_msg_len} å­—ç¬¦æœªæ˜¾ç¤º"
            self.post_message(
                mtype=NotificationType.SiteMessage,
                title=f"ã€è±†ç“£è¯„åˆ†ä¿®æ­£ã€‘åŒæ­¥å®Œæˆï¼ˆæ›´æ–° {updated_count} éƒ¨ï¼‰",
                text=message
            )

    def get_form(self) -> Tuple[List[dict], Dict[str, Any]]:
        """é…ç½®è¡¨å•ï¼ˆæ–°å¢é…ç½®é¡¹ï¼‰"""
        return [
            {
                "component": "VForm",
                "content": [
                    # åŸºç¡€é…ç½®è¡Œ
                    {
                        "component": "VRow",
                        "content": [
                            {
                                "component": "VCol",
                                "props": {"cols": 12, "md": 3},
                                "content": [
                                    {
                                        "component": "VSwitch",
                                        "props": {"model": "enabled", "label": "å¯ç”¨æ’ä»¶", "color": "primary"}
                                    }
                                ]
                            },
                            {
                                "component": "VCol",
                                "props": {"cols": 12, "md": 3},
                                "content": [
                                    {
                                        "component": "VSwitch",
                                        "props": {"model": "notify", "label": "åŒæ­¥ç»“æœé€šçŸ¥"}
                                    }
                                ]
                            },
                            {
                                "component": "VCol",
                                "props": {"cols": 12, "md": 3},
                                "content": [
                                    {
                                        "component": "VSwitch",
                                        "props": {"model": "onlyonce", "label": "ç«‹å³åŒæ­¥ä¸€æ¬¡"}
                                    }
                                ]
                            },
                            {
                                "component": "VCol",
                                "props": {"cols": 12, "md": 3},
                                "content": [
                                    {
                                        "component": "VTextField",
                                        "props": {
                                            "model": "max_sync_count",
                                            "label": "å•æ¬¡åŒæ­¥æœ€å¤§æ•°é‡",
                                            "type": "number",
                                            "min": 10,
                                            "max": 500,
                                            "placeholder": "é»˜è®¤ï¼š100"
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    # åŒæ­¥ç±»å‹ç­›é€‰è¡Œ
                    {
                        "component": "VRow",
                        "content": [
                            {
                                "component": "VCol",
                                "props": {"cols": 12, "md": 6},
                                "content": [
                                    {
                                        "component": "VSelect",
                                        "props": {
                                            "model": "sync_types",
                                            "label": "åŒæ­¥ç±»å‹ç­›é€‰",
                                            "multiple": True,
                                            "items": [
                                                {"label": "ç”µå½±", "value": "movie"},
                                                {"label": "ç”µè§†å‰§/ç»¼è‰º/åŠ¨æ¼«", "value": "tv"}
                                            ],
                                            "placeholder": "é»˜è®¤ï¼šç”µå½±+ç”µè§†å‰§/ç»¼è‰º/åŠ¨æ¼«"
                                        }
                                    }
                                ]
                            },
                            {
                                "component": "VCol",
                                "props": {"cols": 12, "md": 6},
                                "content": [
                                    {
                                        "component": "VTextField",
                                        "props": {
                                            "model": "douban_score_update_days",
                                            "label": "è¯„åˆ†æ›´æ–°å‘¨æœŸï¼ˆå¤©ï¼‰",
                                            "type": "number",
                                            "min": 0,
                                            "placeholder": "é»˜è®¤ï¼š30å¤©",
                                            "hint": "0è¡¨ç¤ºä»…é¦–æ¬¡è·å–ï¼Œä¸è‡ªåŠ¨æ›´æ–°"
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    # å®šæ—¶é…ç½®è¡Œ
                    {
                        "component": "VRow",
                        "content": [
                            {
                                "component": "VCol",
                                "props": {"cols": 12},
                                "content": [
                                    {
                                        "component": "VTextField",
                                        "props": {
                                            "model": "cron",
                                            "label": "å®šæ—¶åŒæ­¥å‘¨æœŸï¼ˆCronè¡¨è¾¾å¼ï¼‰",
                                            "placeholder": "é»˜è®¤ï¼š0 1 * * *ï¼ˆæ¯å¤©å‡Œæ™¨1ç‚¹ï¼‰",
                                            "hint": "Cronæ ¼å¼ï¼šåˆ† æ—¶ æ—¥ æœˆ å‘¨ï¼Œä¾‹å¦‚ 0 3 * * * æ¯å¤©3ç‚¹"
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    # Cookieé…ç½®è¡Œ
                    {
                        "component": "VRow",
                        "content": [
                            {
                                "component": "VCol",
                                "props": {"cols": 12},
                                "content": [
                                    {
                                        "component": "VTextarea",
                                        "props": {
                                            "model": "cookie",
                                            "label": "è±†ç“£Cookieï¼ˆå¯é€‰ï¼‰",
                                            "rows": 2,
                                            "placeholder": "ç•™ç©ºåˆ™ä»CookieCloudè·å–ï¼Œæ ¼å¼ï¼šname1=value1; name2=value2",
                                            "hint": "å»ºè®®é…ç½®ï¼Œé¿å…è±†ç“£é™æµ"
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    # æ•°æ®åº“è·¯å¾„è¡Œ
                    {
                        "component": "VRow",
                        "content": [
                            {
                                "component": "VCol",
                                "props": {"cols": 12},
                                "content": [
                                    {
                                        "component": "VTextarea",
                                        "props": {
                                            "model": "db_path",
                                            "label": "æå½±è§†æ•°æ®åº“è·¯å¾„ï¼ˆå¿…å¡«ï¼‰",
                                            "rows": 1,
                                            "placeholder": "ç¤ºä¾‹ï¼š/zspace/zsrp/sqlite/zvideo/zvideo.db",
                                            "hint": "éœ€é€šè¿‡Portainer/1Panelæ˜ å°„æå½±è§†æ•°æ®åº“æ–‡ä»¶åˆ°å®¹å™¨å¯è®¿é—®è·¯å¾„"
                                        }
                                    }
                                ]
                            }
                        ]
                    },
                    # æç¤ºè¡Œ
                    {
                        "component": "VRow",
                        "content": [
                            {
                                "component": "VCol",
                                "props": {"cols": 12},
                                "content": [
                                    {
                                        "component": "VAlert",
                                        "props": {
                                            "type": "info",
                                            "variant": "tonal",
                                            "text": "âš ï¸ é‡è¦æç¤ºï¼š1. ä½¿ç”¨å‰è¯·å¤‡ä»½æå½±è§†æ•°æ®åº“ï¼›2. é…ç½®Cookieå¯é™ä½é™æµé£é™©ï¼›3. åŒæ­¥ç±»å‹å»ºè®®æŒ‰éœ€å‹¾é€‰ï¼Œæå‡æ•ˆç‡"
                                        }
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }
        ], {
            "enabled": False,
            "notify": True,
            "onlyonce": False,
            "cron": "0 1 * * *",
            "douban_score_update_days": 30,
            "max_sync_count": 100,
            "sync_types": ["movie", "tv"],
            "cookie": "",
            "db_path": ""
        }

    def get_page(self) -> List[dict]:
        """æ— éœ€è¯¦æƒ…é¡µé¢ï¼Œè¿”å›ç©º"""
        return []

    def stop_service(self):
        """åœæ­¢å®šæ—¶ä»»åŠ¡"""
        self._should_stop = True
        try:
            if self._scheduler:
                self._scheduler.remove_all_jobs()
                if self._scheduler.running:
                    self._scheduler.shutdown()
                self._scheduler = None
        except Exception as e:
            logger.error(f"åœæ­¢æœåŠ¡å¤±è´¥ï¼š{e}")


if __name__ == "__main__":
    # æœ¬åœ°æµ‹è¯•ä»£ç 
    plugin = DoubanRatingSync()
    plugin.init_plugin({
        "enabled": True,
        "onlyonce": True,
        "db_path": "/path/to/zvideo.db",
        "cookie": "your_douban_cookie",
        "notify": True,
        "max_sync_count": 50,
        "sync_types": ["movie", "tv"],
        "douban_score_update_days": 30
    })